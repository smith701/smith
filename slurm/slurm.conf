#
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName=msf

ControlMachine=localhost
ControlAddr=localhost

#BackupController=luc
#BackupAddr=37.187.155.184

#ControlAddr=msfdev-frontal2
#BackupAddr=msfdev-frontal2
#
SlurmUser=slurm
#SlurmdUser=root
SlurmctldPort=16817
SlurmdPort=16818
#AuthType=auth/none
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation=/home/MSF/slurm/spool/state
SlurmdSpoolDir=/home/MSF/slurm/spool
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/pgid
BatchStartTimeout=30
MessageTimeout=60
JobRequeue=1
GetEnvTimeout=10
#PluginDir=
CacheGroups=0
#FirstJobId=
#mlm remet en service un slurmd qui a été mis désactivé par le controleur
ReturnToService=0
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#Prolog=
#Epilog=
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
#TaskPlugin=
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=
#
# TIMERS
SlurmctldTimeout=2
SlurmdTimeout=30
InactiveLimit=0
MinJobAge=3000
KillWait=3
Waittime=0
#
# SCHEDULING
SchedulerType=sched/backfill
#SchedulerAuth=
#SchedulerPort=
#SchedulerRootFilter=

#Scheduling Policies
PreemptType=preempt/none
#PreemptType=preempt/partition_prio
#PreemptMode=GANG,SUSPEND


#
# "SelectType"         : node selection logic for scheduling.
#    "select/cons_res" : allocate individual consumable resources
#                        (i.e. processors, memory, etc.)
#
SelectType=select/cons_res
#
# o Define parameters to describe the SelectType plugin. For
#    - select/bluegene - this parameter is currently ignored
#    - select/linear   - this parameter is currently ignored
#    - select/cons_res - the parameters available are
#       - CR_CPU  (1)  - CPUs as consumable resources.
#                        No notion of sockets, cores, or threads.
#                        On a multi-core system CPUs will be cores
#                        On a multi-core/hyperthread system CPUs
#                                        will be threads
#                        On a single-core systems CPUs are CPUs.
#      - CR_Socket (2) - Sockets as a consumable resource.
#      - CR_Core   (3) - Cores as a consumable resource.
#      - CR_Memory (4) - Memory as a consumable resource.
#                        Note! CR_Memory assumes Shared=Yes
#      - CR_Socket_Memory (5) - Socket and Memory as consumable
#                               resources.
#      - CR_Core_Memory (6)   - Core and Memory as consumable
#                               resources. (Not yet implemented)
#      - CR_CPU_Memory (7)    - CPU and Memory as consumable
#                               resources.
SelectTypeParameters=CR_LLN,CR_Core
#
# SchedulerParameters
#       default_queue_depth=#
#              The default number of jobs to  attempt  scheduling  (i.e.
#              the  queue  depth)  when a running job completes or other
#              routine actions occur. The full queue will be tested on a
#              less  frequent  basis.  The default value is 100.  In the
#              case of large clusters (more than 1000 nodes),  configur‐
#              ing a relatively small value may be desirable.
#
#       defer  Setting  this  option  will  avoid attempting to schedule
#              each job individually at job submit time,  but  defer  it
#              until a later time when scheduling multiple jobs simulta‐
#              neously may be possible.  This option may improve  system
#              responsiveness when large numbers of jobs (many hundreds)
#              are submitted at the same time, but  it  will  delay  the
#              initiation    time   of   individual   jobs.   Also   see
#              default_queue_depth above.
#
#       bf_interval=#
#              The number of seconds between iterations.  Higher  values
#              result  in  less overhead and better responsiveness.  The
#              default value is 30 seconds.  This option applies only to
#              SchedulerType=sched/backfill.
#
#       bf_resolution=#
#              The  number  of  seconds  in the resolution of data main‐
#              tained about when jobs  begin  and  end.   Higher  values
#              result  in  less overhead and better responsiveness.  The
#              default value is 60 seconds.  This option applies only to
#              SchedulerType=sched/backfill.
#
#       bf_window=#
#              The  number  of minutes into the future to look when con‐
#              sidering jobs to schedule.  Higher values result in  more
#              overhead  and  less responsiveness.  The default value is
#              1440 minutes (one day).   This  option  applies  only  to
#              SchedulerType=sched/backfill.
#
#       max_job_bf=#
#              The maximum number of jobs to attempt backfill scheduling
#              for (i.e. the queue depth).  Higher values result in more
#              overhead  and  less  responsiveness.  Until an attempt is
#              made to backfill schedule a job, its expected  initiation
#              time value will not be set.  The default value is 50.  In
#              the case of large clusters (more than 1000 nodes) config‐
#              ured with SelectType=select/cons_res, configuring a rela‐
#              tively small value may be desirable.  This option applies
#              only to SchedulerType=sched/backfill.
#
#SchedulerParameters=default_queue_depth=2000,defer,max_job_bf=1000,bf_window=500
#mis a 2 pour qu'un noeud ne pass pas a DOWN en cas de pb de ressource 
FastSchedule=2

PriorityType=priority/multifactor
PriorityDecayHalfLife=12:00:00
PriorityUsageResetPeriod=DAILY 
PriorityWeightFairshare=1000
PriorityWeightAge=10
PriorityWeightPartition=0
PriorityWeightJobSize=0
#PriorityMaxAge=1-0
#
# LOGGING
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurm/slurmd.log


JobCompType=jobcomp/none
#JobCompLoc=
#
# ACCOUNTING
#JobAcctGatherType=jobacct_gather/linux
#JobAcctGatherFrequency=30
#
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=localhost
AccountingStorageLoc=/home/MSF/slurm/slurm_job_accounting.txt
AccountingStorageEnforce=associations,limits,qos
#AccountingStoragePass=
#AccountingStorageUser=
#
# COMPUTE NODES
# COMPUTE NODES
# DD: STRONG WARNING
# Bullshit here:
# 1. slurm sorts node names in alphabetic order at some point.
# If nodes are not declared here in alphabetic order, slurm screws up everything
# and assign properties of one node to another one => guaranteed disaster.
# The problem is mentionned in some mailing list, but apparently not solved.
# 2. Memory sizes must be slightly smaller than the real memory available on the nodes, otherwise,
# the node starts, but is rapidly disbaled because of "LowRealMemory".

NodeName=localhost Sockets=1 CoresPerSocket=2 ThreadsPerCore=1 RealMemory=7000 State=UNKNOWN

PartitionName=DEFAULT Shared=FORCE:1 Nodes=localhost MaxTime=INFINITE State=UP
PartitionName=pri_verylow   Priority=1 PreemptMode=suspend 
PartitionName=pri_low       Priority=2 PreemptMode=suspend 
PartitionName=pri_normal    Priority=3 PreemptMode=suspend Default=YES 
PartitionName=pri_high      Priority=4 PreemptMode=suspend 
PartitionName=pri_veryhigh  Priority=5 PreemptMode=off


